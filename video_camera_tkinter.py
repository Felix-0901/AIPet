import cv2
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import time
from ultralytics import YOLO
import numpy as np
import logging
import queue
import asyncio
from bleak import BleakClient, BleakScanner
import threading
import re
import openai

logging.getLogger("ultralytics").setLevel(logging.WARNING)

# HC-08 çš„ UUID
HC08_SERVICE_UUID = "0000ffe0-0000-1000-8000-00805f9b34fb"
HC08_CHARACTERISTIC_UUID = "0000ffe1-0000-1000-8000-00805f9b34fb"

# HC-08 è£ç½®ä½å€
# hc08_address = "9E97AD80-B02C-477C-875B-AE56EE4DB54F"
hc08_address = "A838409C-77BB-F4E9-997C-A19DBF30BC7E"

# å»ºç«‹ queue è®“ Thread å‚³æ•¸æ“šå›ä¸»åŸ·è¡Œç·’
bluetooth_queue = queue.Queue()

# åˆå§‹åŒ– Tkinter è¦–çª—
root = tk.Tk()
root.title("YOLO Webcam Detection")

# è¨­å®šç•«é¢å¤§å°
canvas_width = 800
canvas_height = 400

# å‰µå»ºç•«å¸ƒä¾†é¡¯ç¤ºå½±ç‰‡
canvas = tk.Canvas(root, width=canvas_width, height=canvas_height)
canvas.pack()

# åŠ è¼‰ YOLO æ¨¡å‹ 11 you only look once
model = YOLO("model.onnx")

# è¨­å®š YOLO åƒæ•¸
pred_args = {
    "batch": 16,
    "imgsz": 640,
    "conf": 0.85,
}

# é–‹å•Ÿæ”å½±æ©Ÿ
cap = cv2.VideoCapture(0)

# å»ºç«‹ log è¨Šæ¯å€åŸŸ
log_frame = tk.Frame(root)
log_frame.pack(fill=tk.BOTH, expand=True)

status_label = tk.Label(log_frame, text="AIè¨Šæ¯", font=("Arial", 12, "bold"))
status_label.pack()

log_text = tk.Text(log_frame, height=10, width=5, wrap=tk.WORD)
log_text.pack(fill=tk.BOTH, expand=True)
# log_text.insert(tk.END, "AIè¨Šæ¯\n")  # åˆå§‹è¨Šæ¯

# ç‹€æ…‹å€åŸŸ (é¡¯ç¤ºç§»å‹• & éœæ­¢æ™‚é–“)
status_frame = tk.Frame(root)
status_frame.pack(fill=tk.BOTH, expand=True)

status_label = tk.Label(status_frame, text="å¯µç‰©ç‹€æ…‹", font=("Arial", 12, "bold"))
status_label.pack()

status_text = tk.Text(status_frame, height=5, wrap=tk.WORD)
status_text.pack(fill=tk.BOTH, expand=True)

# ç”¨æ–¼è¨˜éŒ„ç‰©ä»¶çš„ç§»å‹•ç‹€æ…‹
object_status = {}  # key: class_id, value: {"position": (x, y), "moving_time": float, "stationary_time": float, "last_update": float}
temperature = 0
humidity = 0
animal = None

def log_message(message):
    """åœ¨ log å€åŸŸé¡¯ç¤ºè¨Šæ¯"""
    log_text.insert(tk.END, f"{message}\n")
    log_text.see(tk.END)  # è‡ªå‹•æ²å‹•åˆ°åº•éƒ¨

def update_status_display():
    """æ›´æ–°ç‹€æ…‹å€åŸŸçš„é¡¯ç¤º"""
    status_text.delete(1.0, tk.END)  # æ¸…ç©ºç‹€æ…‹é¡¯ç¤º
    if not object_status:
        status_text.insert(tk.END, "ç›®å‰æœªåµæ¸¬åˆ°ç‰©ä»¶")
        return

    for class_id, info in object_status.items():
        class_name = model.names.get(class_id, "Unknown")
        moving_time = info["moving_time"]
        stationary_time = info["stationary_time"]
        status_text.insert(tk.END, f"ç‰©ä»¶: {class_name} | ç´¯ç©ç§»å‹•: {moving_time:.1f}s | ç´¯ç©éœæ­¢: {stationary_time:.1f}s\n")

def update_frame():
    success, frame = cap.read()
    if success:
        flipped_frame = cv2.flip(frame, 1)
        results = model(flipped_frame, **pred_args)
        annotated_frame = results[0].plot()

        # è½‰æ› OpenCV å½±åƒç‚º PIL å½±åƒ
        image = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)
        image = Image.fromarray(image)
        image = image.resize((canvas_width, canvas_height))
        photo = ImageTk.PhotoImage(image)

        # æ›´æ–°ç•«å¸ƒå½±åƒ
        canvas.create_image(0, 0, anchor=tk.NW, image=photo)
        canvas.image = photo

        # æ›´æ–°ç‰©ä»¶ç‹€æ…‹
        current_time = time.time()
        detection_threshold = 10  # åº§æ¨™è®ŠåŒ–é–¾å€¼ï¼Œè¶…éè¦–ç‚ºç§»å‹•

        if hasattr(results[0], "boxes") and results[0].boxes is not None:
            for box in results[0].boxes:
                values = box.xyxy[0].tolist()  # [x1, y1, x2, y2]
                x1, y1, x2, y2 = values
                class_id = int(box.cls[0].item())  # ç‰©ä»¶é¡åˆ¥ ID
                class_name = model.names.get(class_id, "Unknown")
                global animal
                if animal is None:
                    animal = class_name
                    print(f"animal: {class_name}")

                # è¨ˆç®—ç‰©ä»¶ä¸­å¿ƒé»åº§æ¨™
                cx, cy = (x1 + x2) / 2, (y1 + y2) / 2

                if class_id not in object_status:
                    # åˆå§‹åŒ–è©²ç‰©ä»¶çš„ç‹€æ…‹
                    object_status[class_id] = {
                        "position": (cx, cy),
                        "moving_time": 0.0,
                        "stationary_time": 0.0,
                        "last_update": current_time
                    }
                else:
                    prev_cx, prev_cy = object_status[class_id]["position"]
                    distance = np.sqrt((cx - prev_cx) ** 2 + (cy - prev_cy) ** 2)  # è¨ˆç®—è·é›¢è®ŠåŒ–
                    elapsed_time = current_time - object_status[class_id]["last_update"]

                    if distance > detection_threshold:
                        # ç‰©ä»¶æ­£åœ¨ç§»å‹• -> ç´¯ç©ç§»å‹•æ™‚é–“
                        object_status[class_id]["moving_time"] += elapsed_time
                    else:
                        # ç‰©ä»¶ä¿æŒéœæ­¢ -> ç´¯ç©éœæ­¢æ™‚é–“
                        object_status[class_id]["stationary_time"] += elapsed_time

                    # æ›´æ–°åº§æ¨™ & æ™‚é–“æˆ³è¨˜
                    object_status[class_id]["position"] = (cx, cy)
                    object_status[class_id]["last_update"] = current_time

        update_status_display()  # æ›´æ–°ç‹€æ…‹å€åŸŸ

    # è¨­å®šæ¯ 10ms æ›´æ–°ä¸€æ¬¡ç•«é¢
    root.after(10, update_frame)

def capture_image():
    success, frame = cap.read()
    if success:
        flipped_frame = cv2.flip(frame, 1)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"capture_{timestamp}.png"
        cv2.imwrite(filename, flipped_frame)
        log_message(f"æ“·å–ç•«é¢: {filename}")

def image_recognition():
    log_message("=== AIè¾¨è­˜çµæœ ===")
    if not object_status:
        log_message("ç›®å‰æœªåµæ¸¬åˆ°ç‰©ä»¶")
        return

    for class_id, info in object_status.items():
        class_name = model.names.get(class_id, "Unknown")
        moving_time = info["moving_time"]
        stationary_time = info["stationary_time"]
        # log_message(f"ç‰©ä»¶: {class_name} | ç´¯ç©ç§»å‹•: {moving_time:.1f}s | ç´¯ç©éœæ­¢: {stationary_time:.1f}s")

        def api_call():
            global temperature
            global humidity
            if temperature is None or temperature == 0:
                temperature = 25
            if humidity is None or humidity == 0:
                humidity = 25
            openai.api_key = "sk-6mSHBLj3QjA49HtK6756985c692c496dBd2764Ea42B60b41"
            openai.base_url = "https://free.v36.cm/v1/"
            openai.default_headers = {"x-foo": "true"}
            completion = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {
                        "role": "user",
                        "content": f"è«‹å‘Šè¨´æˆ‘é£¼é¤Šçš„å¯µç‰©{class_name}é©åˆåœ¨æº«åº¦{temperature}, æ¿•åº¦{humidity}çš„ç’°å¢ƒä¸‹æ´»å‹•å—ï¼Ÿå°æ–¼é€™æ¨£çš„é£¼é¤Šç’°å¢ƒæœ‰ä½•å»ºè­°ï¼Ÿ" +
                                   f"{class_name}ä¸€å¤©çš„æ´»å‹•æ™‚é–“ç‚º{moving_time * 3600}ç§’ï¼Œä¼‘æ¯æ™‚é–“ç‚º{stationary_time * 3600}ç§’æ˜¯å¥åº·çš„å—ï¼Ÿ",
                    },
                ],
            )
            print(
                f"Ask AI : è«‹å‘Šè¨´æˆ‘é£¼é¤Šçš„å¯µç‰©{class_name}é©åˆåœ¨æº«åº¦{temperature}, æ¿•åº¦{humidity}çš„ç’°å¢ƒä¸‹æ´»å‹•å—ï¼Ÿå°æ–¼é€™æ¨£çš„é£¼é¤Šç’°å¢ƒæœ‰ä½•å»ºè­°ï¼Ÿ" +
                f"{class_name}ä¸€å¤©çš„æ´»å‹•æ™‚é–“ç‚º{moving_time * 3600}ç§’ï¼Œä¼‘æ¯æ™‚é–“ç‚º{stationary_time * 3600}ç§’æ˜¯å¥åº·çš„å—ï¼Ÿ")
            log_message(completion.choices[0].message.content)

        threading.Thread(target=api_call, daemon=True).start()


def process_bluetooth_data():
    while not bluetooth_queue.empty():
        data = bluetooth_queue.get()
        print(f"ğŸ“¡ æ”¶åˆ°è—ç‰™æ•¸æ“š: {data}")
        temp_match = re.search(r"æº«åº¦:\s*([\d.]+)", data)
        humidity_match = re.search(r"æ¿•åº¦:\s*([\d.]+)%", data)
        global temperature
        global humidity
        temperature_val = float(temp_match.group(1)) if temp_match else None
        humidity_val = float(humidity_match.group(1)) if humidity_match else None
        if temp_match:
            temperature = temperature_val
            print(f"è§£æå‡ºçš„æº«åº¦: {temperature}Â°C")
        if humidity_match:
            humidity = humidity_val
            print(f"è§£æå‡ºçš„æ¿•åº¦: {humidity}%")
    root.after(100, process_bluetooth_data)


def bluetooth_listener():
    print("ğŸ” æƒæè—ç‰™è¨­å‚™...")
    devices = asyncio.run(BleakScanner.discover())
    print(f"ğŸ” ç™¼ç¾ {len(devices)} å€‹è£ç½®")

    hc08_address = "9E97AD80-B02C-477C-875B-AE56EE4DB54F"
    for device in devices:
        if "HC-08" in (device.name or ""):
            hc08_address = device.address
            print(f"ğŸ¯ æ‰¾åˆ° HC-08: {hc08_address}")
            break

    print("ğŸ”— å˜—è©¦é€£æ¥ HC-08...")
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    async def connect():
        async with BleakClient(hc08_address) as client:
            print(f"âœ… å·²é€£ç·šåˆ° {hc08_address}")

            def notification_handler(sender, data):
                message = data.decode("utf-8")
                bluetooth_queue.put(message)  # æŠŠæ•¸æ“šæ”¾å…¥ queue

            await client.start_notify(HC08_CHARACTERISTIC_UUID, notification_handler)

            while True:
                await asyncio.sleep(1)

    loop.run_until_complete(connect())

# å‰µå»ºä¸€å€‹æ°´å¹³æ’åˆ—çš„æ¡†æ¶ä¾†å®¹ç´æŒ‰éˆ•
button_frame = tk.Frame(root)
button_frame.pack(pady=10)

# å‰µå»ºã€Œæ“·å–ç•«é¢ã€æŒ‰éˆ•
# capture_button = ttk.Button(button_frame, text="æ“·å–ç•«é¢", command=capture_image)
# capture_button.pack(side=tk.LEFT, padx=10)

# å‰µå»ºã€Œå½±åƒè¾¨è­˜ã€æŒ‰éˆ•
test_button = ttk.Button(button_frame, text="AIåˆ†æç…§é¡§", command=image_recognition)
test_button.pack(side=tk.LEFT, padx=10)


bt_thread = threading.Thread(target=bluetooth_listener, daemon=True)
bt_thread.start()

# å•Ÿå‹•ç•«é¢æ›´æ–°
update_frame()

# å•Ÿå‹•è—èŠ½
process_bluetooth_data()

# é‹è¡Œ Tkinter äº‹ä»¶è¿´åœˆ
root.mainloop()

# é‡‹æ”¾æ”å½±æ©Ÿè³‡æº
cap.release()
cv2.destroyAllWindows()
